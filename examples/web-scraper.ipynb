{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbCGhkCl5sZR"
   },
   "source": [
    "# Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMjfeD0n5sZX"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import os\n",
    "import openpyxl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperBase(object):\n",
    "    def __init__(self, webdriver: webdriver, name: str, web_url: str) -> None:\n",
    "        self._webdriver = webdriver\n",
    "        self._name = name\n",
    "        self._web_url = web_url\n",
    "        self._storage = {}\n",
    "    \n",
    "    def _search_desired_item(self) -> None:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _get_all_product_links(self) -> List[str]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _parse_data(self, link: str) -> None:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def run(self) -> Dict:\n",
    "        start_time = time.time()\n",
    "        self._webdriver.get(self._web_url)\n",
    "        self._search_desired_item()\n",
    "        links = self._get_all_product_links()\n",
    "        print(\"Num of links: %d\" % len(links))\n",
    "        for i in range(len(links)):\n",
    "            self._parse_data(link=links[i])\n",
    "            print(\"Progress: %d/%d\" % (i + 1, len(links)), end='\\r' )\n",
    "        print(\"Progress: %d/%d\" % (len(links), len(links)))\n",
    "        print(\"Total time elapsed: %.2fs\" % (time.time() - start_time))\n",
    "        return self._storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EbayScraper(ScraperBase):\n",
    "    def __init__(self, webdriver: webdriver, name: str, web_url: str) -> None:\n",
    "        super(EbayScraper, self).__init__(webdriver=webdriver, name=name, web_url=web_url)\n",
    "\n",
    "    def _search_desired_item(self) -> None:\n",
    "        search_bar = self._webdriver.find_element_by_xpath('//*[@id=\"gh-ac\"]')\n",
    "        search_bar.send_keys(\"laptop\", Keys.ENTER)\n",
    "        refurbished_items = self._webdriver.find_element_by_xpath(\n",
    "            '//*[@id=\"x-refine__group__2\"]/ul/li[3]/div/a/div/div/span[1]')\n",
    "        refurbished_items.click()\n",
    "        non_auction_items = self._webdriver.find_element_by_xpath(\n",
    "            '//*[@id=\"s0-14-11-5-1[0]\"]/div[2]/div/div/ul/li[3]/a/h2')\n",
    "        non_auction_items.click()\n",
    "        self._webdriver.get(self._webdriver.current_url + \"&_ipg=200\")\n",
    "    \n",
    "    def _get_all_product_links(self) -> List[str]:\n",
    "        link_container = self._webdriver.find_element_by_xpath(\n",
    "            '//*[@id=\"srp-river-results\"]/ul').find_elements_by_tag_name(\"a\")\n",
    "        links = []\n",
    "        for link in link_container:\n",
    "            links.append(link.get_attribute(\"href\"))\n",
    "        links = list(set(links))\n",
    "        return links\n",
    "    \n",
    "    def _parse_data(self, link: str) -> None:\n",
    "        self._webdriver.get(link)\n",
    "        product_id = str(len(self._storage.keys()) + 1)\n",
    "        try:\n",
    "            product_title = self._webdriver.find_element_by_xpath('//*[@id=\"itemTitle\"]').text\n",
    "            product_cost = self._webdriver.find_element_by_xpath('//*[@id=\"prcIsum\"]').get_attribute(\"content\")\n",
    "            specs = self._webdriver.find_element_by_xpath(\n",
    "                '//*[@id=\"viTabs_0_is\"]/div/table').find_elements_by_tag_name(\"tr\")\n",
    "            try:\n",
    "                product_desc = self._webdriver.find_element_by_xpath(\n",
    "                    '//*[@id=\"ds_div\"]/font/font/font/font/font/font/font/font/font/ul/li/p').text\n",
    "            except NoSuchElementException:\n",
    "                product_desc = \"\"\n",
    "            self._storage[product_id] = {\"title\": product_title, 'link': link, \"price\": product_cost, \n",
    "                                         \"description\": product_desc}\n",
    "            for spec in specs:\n",
    "                spec_titles = spec.find_elements_by_class_name(\"attrLabels\")\n",
    "                spec_values = spec.find_elements_by_tag_name(\"span\")\n",
    "                if len(spec_titles) == len(spec_values):\n",
    "                    for j in range(len(spec_titles)):\n",
    "                        self._storage[product_id][spec_titles[j].text.strip()] = spec_values[j].text.strip()\n",
    "        except NoSuchElementException:\n",
    "            print(\"\\n\\rParsing failed, link: %s\" % link)\n",
    "            self._storage.pop(product_id, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReebeloScraper(ScraperBase):\n",
    "    def __init__(self, webdriver: webdriver, name: str, web_url: str) -> None:\n",
    "        super(ReebeloScraper, self).__init__(webdriver=webdriver, name=name, web_url=web_url)\n",
    "    \n",
    "    def _search_desired_item(self) -> None:\n",
    "        search_bar = self._webdriver.find_element_by_xpath('//*[@id=\"bc-sf-search-box-0\"]')\n",
    "        search_bar.send_keys(\"laptop\", Keys.ENTER)\n",
    "        time.sleep(3)\n",
    "        self._webdriver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    def _get_all_product_links(self) -> List[str]:\n",
    "        link_container = self._webdriver.find_element_by_xpath(\n",
    "            '//*[@id=\"bc-sf-filter-products\"]').find_elements_by_tag_name(\"a\")\n",
    "        links = []\n",
    "        for link in link_container:\n",
    "            links.append(link.get_attribute(\"href\"))\n",
    "        links = list(set(links))\n",
    "        return links\n",
    "    \n",
    "    def _parse_data(self, link: str) -> None:\n",
    "        self._webdriver.get(link)\n",
    "        product_id = str(len(self._storage.keys()) + 1)\n",
    "        product_title = self._webdriver.find_element_by_xpath(\n",
    "                '/html/body/main/div[2]/section/div[1]/div[3]/div/div[2]/div/div[1]/form/div[1]/div[1]/h1').text\n",
    "        product_cost = self._webdriver.find_element_by_xpath(\n",
    "            '/html/body/main/div[2]/section/div[1]/div[3]/div/div[2]/div/div[1]/form/div[1]/div[2]/div/'\n",
    "            'div[1]/span[1]/span').text\n",
    "        self._storage[product_id] = {\"title\": product_title, 'link': link, \"price\": product_cost, \n",
    "                                     \"description\": \"\"}\n",
    "        try:\n",
    "            view_more_btn = self._webdriver.find_element_by_class_name(\"cus-btn\").find_element_by_tag_name(\"a\")\n",
    "            view_more_btn.click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        try:\n",
    "            specs = self._webdriver.find_element_by_class_name(\"table-wrapper\").find_elements_by_tag_name(\"tr\")\n",
    "        except NoSuchElementException:\n",
    "            specs = self._webdriver.find_element_by_class_name(\"custom-table\").find_elements_by_tag_name(\"li\")\n",
    "            for spec in specs:\n",
    "                parsed_spec = spec.text.split(\":\")\n",
    "                if len(parsed_spec) == 2:\n",
    "                    spec_title = parsed_spec[0].strip()\n",
    "                    spec_value = parsed_spec[1].strip()\n",
    "                    self._storage[product_id][spec_title] = spec_value\n",
    "                elif len(parsed_spec) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise Exception(\"parsed_spec: %s\" % parsed_spec)\n",
    "        else:\n",
    "            try:\n",
    "                for spec in specs:\n",
    "                    parsed_spec = spec.find_element_by_tag_name(\"td\").text.split(\":\")\n",
    "                    if len(parsed_spec) == 3:\n",
    "                        spec_title = parsed_spec[0].strip()\n",
    "                        spec_value = parsed_spec[1].split()[0].strip()\n",
    "                        self._storage[product_id][spec_title] = spec_value.strip()\n",
    "                        spec_title = parsed_spec[1][len(spec_value) + 2:].strip()\n",
    "                        spec_value = parsed_spec[2].strip()\n",
    "                    elif len(parsed_spec) == 2:\n",
    "                        spec_title = parsed_spec[0].strip()\n",
    "                        spec_value = parsed_spec[1].strip()\n",
    "                    elif len(parsed_spec) == 1:\n",
    "                        print(parsed_spec[0])\n",
    "                        spec_title, spec_value = \"Model\", parsed_spec[0].strip()\n",
    "                    else:\n",
    "                        raise Exception(\"parsed_spec: %s\" % parsed_spec)\n",
    "                    self._storage[product_id][spec_title] = spec_value\n",
    "            except NoSuchElementException:\n",
    "                print(\"\\n\\rParsing failed, link: %s\" % link)\n",
    "                self._storage.pop(product_id, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(executable_path=os.getenv(\"CHROMEDRIVER_PATH\"), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate all scrapers\n",
    "ebay_scraper = EbayScraper(webdriver=driver, name=\"eBay\", web_url=\"https://www.ebay.com.sg/\")\n",
    "reebelo_scraper = ReebeloScraper(webdriver=driver, name=\"Reebelo\", web_url=\"https://www.reebelo.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start scraping Ebay\n",
    "ebay_storage = ebay_scraper.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start scraping Reebelo\n",
    "reebelo_storage = reebelo_scraper.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the webdriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebay_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reebelo_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ebay = pd.DataFrame.from_dict(ebay_storage, orient='index')\n",
    "df_ebay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reebelo = pd.DataFrame.from_dict(reebelo_storage, orient='index')\n",
    "df_reebelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ebay.to_excel(\"ebay.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reebelo.to_excel(\"reebelo.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lazada example\n",
    "# driver = webdriver.Chrome(executable_path=os.getenv(\"CHROMEDRIVER_PATH\"))\n",
    "# driver.get('https://www.lazada.sg/')\n",
    "\n",
    "# # Search for laptop\n",
    "# search_bar = driver.find_element_by_id(\"q\")\n",
    "# search_bar.send_keys(\"laptop\", Keys.ENTER)\n",
    "\n",
    "# # Filter by Refurbished items\n",
    "# used_checkbox = driver.find_element_by_xpath(\"//span[contains(text(), 'Refurbish')]\")\n",
    "# used_checkbox.click()\n",
    "\n",
    "# link_container = driver.find_element_by_xpath('//*[@id=\"root\"]/div/div[2]/div[1]/div/div[1]/div[3]').find_elements_by_tag_name(\"a\")\n",
    "# links = []\n",
    "# for link in link_container:\n",
    "#     links.append(link.get_attribute(\"href\"))\n",
    "# links = list(set(links))\n",
    "\n",
    "# storage = {}\n",
    "# for i in range(len(links)):\n",
    "#     driver.get(links[i])\n",
    "#     try:\n",
    "#         product_title = driver.find_element_by_class_name(\"pdp-mod-product-badge-title\").text\n",
    "#         specs = driver.find_element_by_class_name(\"specification-keys\").find_elements_by_tag_name(\"li\")\n",
    "#         storage[i] = {\"title\": product_title}\n",
    "#         for spec in specs:\n",
    "#             spec_title = spec.find_element_by_class_name(\"key-title\").text\n",
    "#             storage[i][spec_title] = spec.find_element_by_class_name(\"key-value\").text \n",
    "#     except NoSuchElementException:\n",
    "#         pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Supreme Webscraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
